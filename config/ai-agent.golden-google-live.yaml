# ═══════════════════════════════════════════════════════════════════════════
# Golden Baseline #4: Google Live - Full Agent (Gemini Live)
# ═══════════════════════════════════════════════════════════════════════════
#
# VALIDATED CONFIGURATION - Production-ready
#
# Requirements:
#   - GOOGLE_API_KEY in .env file
#   - Asterisk 18+ with ARI enabled
#
# Performance:
#   - Response latency: <1 second typical (fastest baseline)
#   - Audio quality: Excellent (native 24k output; telephony μ-law target)
#
# Best for:
#   - Lowest-latency interactive voice agents
#   - Native barge-in via provider duplex streaming
#
# For detailed parameter explanations, see: docs/Configuration-Reference.md
# For the full case study, see: docs/case-studies/Google-Live-Golden-Baseline.md
# ═══════════════════════════════════════════════════════════════════════════

config_version: 6
active_pipeline: local_hybrid  # Don't change - uses provider override via contexts
default_provider: google_live

# Audio Transport - AudioSocket for full agent providers
audio_transport: audiosocket
audiosocket:
  format: slin
  host: 0.0.0.0
  port: 8090

# Playback Mode
downstream_mode: stream

# Contexts
contexts:
  default:
    greeting: Hello, how can I help you today?
    profile: telephony_ulaw_8k
    prompt: You are a helpful AI assistant. Be concise and clear.
    provider: google_live
  demo_google_live:
    greeting: "Hi! I'm Ava powered by Google Gemini Live. Try interrupting me. What would you like to know about this project?"
    prompt: "You are Ava (Asterisk Voice Agent) demonstrating Google Gemini Live. Be helpful, conversational, and explain features clearly in 5-10 sentences."
    profile: telephony_ulaw_8k
    provider: google_live
    tools:
      - transfer
      - cancel_transfer
      - hangup_call
      - send_email_summary
      - request_transcript

# Barge-In
barge_in:
  enabled: true
  energy_threshold: 700
  initial_protection_ms: 100
  min_ms: 150
  post_tts_end_protection_ms: 100

# VAD (kept for consistency; provider has native duplex/VAD)
vad:
  enhanced_enabled: true
  fallback_buffer_size: 128000
  fallback_enabled: true
  fallback_interval_ms: 4000
  max_utterance_duration_ms: 10000
  min_utterance_duration_ms: 600
  use_provider_vad: false
  utterance_padding_ms: 200
  webrtc_aggressiveness: 1
  webrtc_end_silence_frames: 50
  webrtc_start_frames: 3

# Streaming
streaming:
  chunk_size_ms: 20
  connection_timeout_ms: 120000
  continuous_stream: true
  empty_backoff_ticks_max: 5
  fallback_timeout_ms: 8000
  greeting_min_start_ms: 40
  jitter_buffer_ms: 950
  keepalive_interval_ms: 5000
  low_watermark_ms: 80
  min_start_ms: 120
  normalizer:
    enabled: true
    max_gain_db: 18.0
    target_rms: 1400
  provider_grace_ms: 500
  sample_rate: 8000

# Audio Profiles
profiles:
  default: telephony_responsive
  telephony_ulaw_8k:
    chunk_ms: auto
    idle_cutoff_ms: 800
    internal_rate_hz: 8000
    provider_pref:
      input_encoding: mulaw
      input_sample_rate_hz: 8000
      output_encoding: mulaw
      output_sample_rate_hz: 8000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000
  telephony_responsive:
    chunk_ms: auto
    idle_cutoff_ms: 600
    internal_rate_hz: 8000
    provider_pref:
      input_encoding: mulaw
      input_sample_rate_hz: 8000
      output_encoding: mulaw
      output_sample_rate_hz: 8000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000

# Provider Configuration - Google Live
providers:
  google_live:
    api_key: ${GOOGLE_API_KEY}
    enabled: true
    type: full
    capabilities:
      - stt
      - llm
      - tts
    continuous_input: true
    greeting: ${GOOGLE_LIVE_GREETING:-Hi! I'm powered by Google Gemini Live API. Try interrupting me!}
    # Telephony input
    input_encoding: ulaw
    input_sample_rate_hz: 8000
    # Provider I/O
    provider_input_encoding: linear16
    provider_input_sample_rate_hz: 16000
    output_encoding: linear16
    output_sample_rate_hz: 24000
    # Telephony target
    target_encoding: ulaw
    target_sample_rate_hz: 8000
    # Model + tuning
    llm_model: gemini-2.5-flash-native-audio-preview-12-2025
    llm_temperature: 0.8
    llm_max_output_tokens: 8192
    llm_top_p: 0.95
    llm_top_k: 40
    # Voice + modalities
    tts_voice_name: Aoede
    response_modalities: audio
    # Transcription
    enable_input_transcription: true
    enable_output_transcription: true

# LLM fallback (unused in full agent mode but required by schema)
llm:
  initial_greeting: Hello, how can I help you today?
  prompt: Voice assistant. Answer in 5-8 words. Be direct. Expand only if asked.

# Pipelines - required but not used by monolithic provider
pipelines:
  local_hybrid:
    llm: openai_llm
    options:
      llm:
        base_url: https://api.openai.com/v1
        max_tokens: 150
        model: gpt-4o-mini
        temperature: 0.7
      stt:
        chunk_ms: 160
        mode: stt
        stream_format: pcm16_16k
        streaming: true
      tts:
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: local_stt
    tts: local_tts

asterisk:
  app_name: asterisk-ai-voice-agent

# ExternalMedia (not used by AudioSocket, but required by config schema)
external_media:
  codec: ulaw
  direction: both
  port_range: 18080:18099
  rtp_host: 0.0.0.0
  rtp_port: 18080

